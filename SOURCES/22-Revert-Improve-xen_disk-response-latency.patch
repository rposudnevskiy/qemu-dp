From cc38773105a6711d7bd96b1841654452a8fc62d9 Mon Sep 17 00:00:00 2001
From: Ronan Abhamon <ronan.abhamon@vates.fr>
Date: Wed, 6 Mar 2019 09:21:02 +0100
Subject: [PATCH 1/1] Revert "Improve xen_disk response latency"

This reverts commit 20706281aed2f9cb753ee0ff15850c8cc2217d95.

Signed-off-by: Ronan Abhamon <ronan.abhamon@vates.fr>
---
 hw/block/xen_disk.c | 47 ++++++++++++++++++++++++++++++---------------
 1 file changed, 31 insertions(+), 16 deletions(-)

diff --git a/hw/block/xen_disk.c b/hw/block/xen_disk.c
index d184934c02..835fef5943 100644
--- a/hw/block/xen_disk.c
+++ b/hw/block/xen_disk.c
@@ -110,9 +110,11 @@ struct XenBlkDev {
 
     /* request lists */
     QLIST_HEAD(inflight_head, ioreq) inflight;
+    QLIST_HEAD(finished_head, ioreq) finished;
     QLIST_HEAD(freelist_head, ioreq) freelist;
     int                 requests_total;
     int                 requests_inflight;
+    int                 requests_finished;
     unsigned int        max_requests;
 
     /* Persistent grants extension */
@@ -132,7 +134,6 @@ struct XenBlkDev {
 /* Threshold of in-flight requests above which we will start using
  * blk_io_plug()/blk_io_unplug() to batch requests */
 #define IO_PLUG_THRESHOLD 1
-static int blk_send_response(struct ioreq *ioreq);
 /* ------------------------------------------------------------- */
 
 static void ioreq_reset(struct ioreq *ioreq)
@@ -232,10 +233,12 @@ static void ioreq_finish(struct ioreq *ioreq)
     struct XenBlkDev *blkdev = ioreq->blkdev;
 
     QLIST_REMOVE(ioreq, list);
+    QLIST_INSERT_HEAD(&blkdev->finished, ioreq, list);
     blkdev->requests_inflight--;
+    blkdev->requests_finished++;
 }
 
-static void ioreq_release(struct ioreq *ioreq)
+static void ioreq_release(struct ioreq *ioreq, bool finish)
 {
     struct XenBlkDev *blkdev = ioreq->blkdev;
 
@@ -243,7 +246,11 @@ static void ioreq_release(struct ioreq *ioreq)
     ioreq_reset(ioreq);
     ioreq->blkdev = blkdev;
     QLIST_INSERT_HEAD(&blkdev->freelist, ioreq, list);
-    blkdev->requests_inflight--;
+    if (finish) {
+        blkdev->requests_finished--;
+    } else {
+        blkdev->requests_inflight--;
+    }
 }
 
 /*
@@ -655,16 +662,6 @@ static void qemu_aio_complete(void *opaque, int ret)
     default:
         break;
     }
-    /* If the I/O ring is full, the guest cannot send any more
-     * requests until some responses are sent. So, whenever an
-     * ioreq completes, just send everything we have ready.
-     * In particular, if this was a read, not sending it at once
-     * just adds read latency for the guest.
-     */
-    if (blk_send_response(ioreq)) {
-        xen_pv_send_notify(&blkdev->xendev);
-    }
-    ioreq_release(ioreq);
     qemu_bh_schedule(blkdev->bh);
 
 done:
@@ -775,7 +772,7 @@ err:
     return -1;
 }
 
-static int blk_send_response(struct ioreq *ioreq)
+static int blk_send_response_one(struct ioreq *ioreq)
 {
     struct XenBlkDev  *blkdev = ioreq->blkdev;
     int               send_notify   = 0;
@@ -824,6 +821,22 @@ static int blk_send_response(struct ioreq *ioreq)
     return send_notify;
 }
 
+/* walk finished list, send outstanding responses, free requests */
+static void blk_send_response_all(struct XenBlkDev *blkdev)
+{
+    struct ioreq *ioreq;
+    int send_notify = 0;
+
+    while (!QLIST_EMPTY(&blkdev->finished)) {
+        ioreq = QLIST_FIRST(&blkdev->finished);
+        send_notify += blk_send_response_one(ioreq);
+        ioreq_release(ioreq, true);
+    }
+    if (send_notify) {
+        xen_pv_send_notify(&blkdev->xendev);
+    }
+}
+
 static int blk_get_request(struct XenBlkDev *blkdev, struct ioreq *ioreq, RING_IDX rc)
 {
     switch (blkdev->protocol) {
@@ -858,6 +871,7 @@ static void blk_handle_requests(struct XenBlkDev *blkdev)
     rp = blkdev->rings.common.sring->req_prod;
     xen_rmb(); /* Ensure we see queued requests up to 'rp'. */
 
+    blk_send_response_all(blkdev);
     /* If there was more than one ioreq in flight when we got here, this
      * is an indication that there the bottleneck is below us, so it's worth
      * beginning to batch up I/O requests rather than submitting them
@@ -899,10 +913,10 @@ static void blk_handle_requests(struct XenBlkDev *blkdev)
                 break;
             };
 
-            if (blk_send_response(ioreq)) {
+            if (blk_send_response_one(ioreq)) {
                 xen_pv_send_notify(&blkdev->xendev);
             }
-            ioreq_release(ioreq);
+            ioreq_release(ioreq, false);
             continue;
         }
 
@@ -946,6 +960,7 @@ static void blk_alloc(struct XenDevice *xendev)
     trace_xen_disk_alloc(xendev->name);
 
     QLIST_INIT(&blkdev->inflight);
+    QLIST_INIT(&blkdev->finished);
     QLIST_INIT(&blkdev->freelist);
 
     if (xen_mode != XEN_EMULATE) {
-- 
2.21.0

