From 94e5512ccfb63365fd6158e9f7321d7cea1f9e55 Mon Sep 17 00:00:00 2001
From: Ronan Abhamon <ronan.abhamon@vates.fr>
Date: Wed, 6 Mar 2019 09:43:33 +0100
Subject: [PATCH 1/1] Revert "Improve xen_disk's batching behaviour"

This reverts commit 598164be269f3fd24f158e2374b46220034766d0.

Signed-off-by: Ronan Abhamon <ronan.abhamon@vates.fr>
---
 hw/block/xen_disk.c | 28 ----------------------------
 1 file changed, 28 deletions(-)

diff --git a/hw/block/xen_disk.c b/hw/block/xen_disk.c
index 835fef5943..1252fb5445 100644
--- a/hw/block/xen_disk.c
+++ b/hw/block/xen_disk.c
@@ -131,9 +131,6 @@ struct XenBlkDev {
     QEMUBH              *bh;
 };
 
-/* Threshold of in-flight requests above which we will start using
- * blk_io_plug()/blk_io_unplug() to batch requests */
-#define IO_PLUG_THRESHOLD 1
 /* ------------------------------------------------------------- */
 
 static void ioreq_reset(struct ioreq *ioreq)
@@ -862,8 +859,6 @@ static void blk_handle_requests(struct XenBlkDev *blkdev)
 {
     RING_IDX rc, rp;
     struct ioreq *ioreq;
-    int inflight_atstart = blkdev->requests_inflight;
-    int batched = 0;
 
     blkdev->more_work = 0;
 
@@ -872,15 +867,6 @@ static void blk_handle_requests(struct XenBlkDev *blkdev)
     xen_rmb(); /* Ensure we see queued requests up to 'rp'. */
 
     blk_send_response_all(blkdev);
-    /* If there was more than one ioreq in flight when we got here, this
-     * is an indication that there the bottleneck is below us, so it's worth
-     * beginning to batch up I/O requests rather than submitting them
-     * immediately. The maximum number of requests we're willing to batch
-     * is the number already in flight, so it can grow up to max_requests
-     * when the bottleneck is below us */
-    if (inflight_atstart > IO_PLUG_THRESHOLD) {
-        blk_io_plug(blkdev->blk);
-    }
     while (rc != rp) {
         /* pull request from ring */
         if (RING_REQUEST_CONS_OVERFLOW(&blkdev->rings.common, rc)) {
@@ -920,21 +906,7 @@ static void blk_handle_requests(struct XenBlkDev *blkdev)
             continue;
         }
 
-        if (inflight_atstart > IO_PLUG_THRESHOLD && batched >= inflight_atstart) {
-            blk_io_unplug(blkdev->blk);
-        }
         ioreq_runio_qemu_aio(ioreq);
-        if (inflight_atstart > IO_PLUG_THRESHOLD) {
-            if (batched >= inflight_atstart) {
-                blk_io_plug(blkdev->blk);
-                batched=0;
-            } else {
-                batched++;
-            }
-        }
-    }
-    if (inflight_atstart > IO_PLUG_THRESHOLD) {
-        blk_io_unplug(blkdev->blk);
     }
 
     if (blkdev->more_work && blkdev->requests_inflight < blkdev->max_requests) {
-- 
2.21.0

