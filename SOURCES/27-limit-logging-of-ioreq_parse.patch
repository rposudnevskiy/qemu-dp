Limit logging of ioreq_parse errors

From: Tim Smith <tim.smith@citrix.com>

If something bad has happened to the ring we will get
a lot of consecutive errors from ioreq_parse() which will
fill the logs and cause much additional trouble.

Count consecutive errors and raise the log level required
if we have too many. This will produce a short burst of
logging every time the counter wraps but that shouldn't
be a problem.

Signed-off-by: Tim Smith <tim.smith@citrix.com>
---
 hw/block/xen_disk.c |   26 ++++++++++++++++++--------
 1 file changed, 18 insertions(+), 8 deletions(-)

diff --git a/hw/block/xen_disk.c b/hw/block/xen_disk.c
index 0d885d1809..b79460ab77 100644
--- a/hw/block/xen_disk.c
+++ b/hw/block/xen_disk.c
@@ -84,6 +84,7 @@ struct XenBlkDev {
     int                 protocol;
     blkif_back_rings_t  rings;
     int                 more_work;
+    unsigned int        errcount;
 
     /* request lists */
     QLIST_HEAD(inflight_head, ioreq) inflight;
@@ -174,6 +175,12 @@ static void ioreq_release(struct ioreq *ioreq)
     blkdev->requests_inflight--;
 }
 
+/* Avoid log flooding of errors by turning them
+ * raising the required log level if we have had too
+ * many consecutive ones. Avoids flooding logs
+ * when the ring goes berserk */
+#define ERT(a) ( ((a->errcount) < 16)?0:3 )
+
 /*
  * translate request into iovec + start offset
  * do sanity checks along the way
@@ -195,36 +202,36 @@ static int ioreq_parse(struct ioreq *ioreq)
     case BLKIF_OP_FLUSH_DISKCACHE:
         ioreq->presync = 1;
         if (!ioreq->req.nr_segments) {
-            return 0;
+            goto out;
         }
         /* fall through */
     case BLKIF_OP_WRITE:
         break;
     case BLKIF_OP_DISCARD:
-        return 0;
+        goto out;
     default:
-        xen_pv_printf(&blkdev->xendev, 0, "error: unknown operation (%d)\n",
+        xen_pv_printf(&blkdev->xendev, ERT(blkdev), "error: unknown operation (%d)\n",
                       ioreq->req.operation);
         goto err;
     };
 
     if (ioreq->req.operation != BLKIF_OP_READ && blkdev->mode[0] != 'w') {
-        xen_pv_printf(&blkdev->xendev, 0, "error: write req for ro device\n");
+        xen_pv_printf(&blkdev->xendev, ERT(blkdev), "error: write req for ro device\n");
         goto err;
     }
 
     ioreq->start = ioreq->req.sector_number * blkdev->file_blk;
     for (i = 0; i < ioreq->req.nr_segments; i++) {
         if (i == BLKIF_MAX_SEGMENTS_PER_REQUEST) {
-            xen_pv_printf(&blkdev->xendev, 0, "error: nr_segments too big\n");
+            xen_pv_printf(&blkdev->xendev, ERT(blkdev), "error: nr_segments too big\n");
             goto err;
         }
         if (ioreq->req.seg[i].first_sect > ioreq->req.seg[i].last_sect) {
-            xen_pv_printf(&blkdev->xendev, 0, "error: first > last sector\n");
+            xen_pv_printf(&blkdev->xendev, ERT(blkdev), "error: first > last sector\n");
             goto err;
         }
         if (ioreq->req.seg[i].last_sect * BLOCK_SIZE >= XC_PAGE_SIZE) {
-            xen_pv_printf(&blkdev->xendev, 0, "error: page crossing\n");
+            xen_pv_printf(&blkdev->xendev, ERT(blkdev), "error: page crossing\n");
             goto err;
         }
 
@@ -235,12 +242,15 @@ static int ioreq_parse(struct ioreq *ioreq)
         qemu_iovec_add(&ioreq->v, (void*)mem, len);
     }
     if (ioreq->start + ioreq->v.size > blkdev->file_size) {
-        xen_pv_printf(&blkdev->xendev, 0, "error: access beyond end of file\n");
+        xen_pv_printf(&blkdev->xendev, ERT(blkdev), "error: access beyond end of file\n");
         goto err;
     }
+out:
+    blkdev->errcount = 0;
     return 0;
 
 err:
+    blkdev->errcount++;
     ioreq->status = BLKIF_RSP_ERROR;
     return -1;
 }
